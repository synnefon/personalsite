const toShavian = require("to-shavian");

// Token types enum
export const TokenType = {
  WORD: "word",
  PUNCTUATION: "punctuation",
  WHITESPACE: "whitespace",
  NEWLINE: "newline",
};

// Shavian shorthand words (single character representations)
const SHAVIAN_SHORTHAND = {
  "ð‘ž": "the",
  "ð‘": "of",
  "ð‘¯": "and",
  "ð‘‘": "to",
};

// Shavian to ARPABET mapping
const SHAVIAN_TO_ARPABET = {
  // Shavian-Only Punctuation
  "Â·": "",

  // Consonants
  "ð‘": "P",
  "ð‘š": "B",
  "ð‘‘": "T",
  "ð‘›": "D",
  "ð‘’": "K",
  "ð‘œ": "G",
  "ð‘“": "F",
  "ð‘": "V",
  "ð‘”": "TH",
  "ð‘ž": "DH",
  "ð‘•": "S",
  "ð‘Ÿ": "Z",
  "ð‘–": "SH",
  "ð‘ ": "ZH",
  "ð‘—": "CH",
  "ð‘¡": "JH",
  "ð‘£": "HH",
  "ð‘¢": "W",
  "ð‘˜": "Y",
  "ð‘®": "R",
  "ð‘¤": "L",
  "ð‘¥": "M",
  "ð‘¯": "N",
  "ð‘™": "NG",

  // Vowels
  "ð‘°": "IY", // eat
  "ð‘¦": "IH", // if
  "ð‘±": "EY", // age
  "ð‘§": "EH", // egg
  "ð‘¨": "AE", // ash
  "ð‘­": "AA", // ah
  "ð‘·": "AO", // awe
  "ð‘´": "OW", // oak
  "ð‘«": "UH", // wool
  "ð‘µ": "UW", // ooze
  "ð‘³": "AH", // up
  "ð‘©": "AX", // schwa
  "ð‘²": "AY", // ice
  "ð‘¬": "AW", // out
  "ð‘¶": "OY", // oil

  // Rhotic Vowels (compound)
  "ð‘»": "ER", // err
  "ð‘¸": "AA+R", // are
  "ð‘¹": "AO+R", // or
  "ð‘º": "EH+R", // air
  "ð‘½": "IH+R", // ear
  "ð‘¼": "AX+R", // schwa + r
  "ð‘¾": "IY+AX", // IY + schwa
  "ð‘¿": "Y+UW", // Y + UW
};

export const shavianateParagraphs = paragraphs =>
  paragraphs.split("\n").map(toShavian).join("\n");

export const getArpabetFromShavian = word => {
  const clean = word.replace(/[^\u{10450}-\u{1047F}]/gu, "");
  const chars = [...clean];
  const arpabet = chars.map(c => SHAVIAN_TO_ARPABET[c] ?? c).join(" ");
  return (chars.length === 1 && SHAVIAN_SHORTHAND[clean])
    ? `${arpabet} (${SHAVIAN_SHORTHAND[clean]})`
    : arpabet;
};

const convertQuotesToGuillemets = (text, quoteState) =>
  text.replace(/["\u201C\u201D]/g, () => {
    const g = quoteState.isOpen ? "Â«" : "Â»";
    quoteState.isOpen = !quoteState.isOpen;
    return g;
  });

const tokenizeLine = (line, tokens, quoteState) => {
  line.split(/(\s+)/).forEach(part => {
    if (!part) return;
    if (/^\s+$/.test(part)) {
      tokens.push({ type: TokenType.WHITESPACE, english: part, shavian: part, index: null });
      return;
    }
    const m = part.match(/^([^\w]*)(\w[\w'\u2019'-]*\w|\w)([^\w]*)$/);
    if (!m) {
      tokens.push({
        type: TokenType.PUNCTUATION,
        english: part,
        shavian: convertQuotesToGuillemets(toShavian(part), quoteState),
        index: null,
      });
      return;
    }
    const [, pre, word, post] = m;
    if (pre) tokens.push({
      type: TokenType.PUNCTUATION,
      english: pre,
      shavian: convertQuotesToGuillemets(toShavian(pre), quoteState),
      index: null,
    });
    const wordIndex = tokens.filter(t => t.type === TokenType.WORD).length;
    tokens.push({
      type: TokenType.WORD,
      english: word,
      shavian: toShavian(word),
      index: wordIndex,
    });
    if (post) tokens.push({
      type: TokenType.PUNCTUATION,
      english: post,
      shavian: convertQuotesToGuillemets(toShavian(post), quoteState),
      index: null,
    });
  });
};

export const tokenizeText = text => {
  const tokens = [];
  const lines = text.split("\n");
  const quoteState = { isOpen: true };
  lines.forEach((line, i) => {
    if (i > 0) tokens.push({
      type: TokenType.NEWLINE, english: "\n", shavian: "\n", index: null
    });
    tokenizeLine(line, tokens, quoteState);
  });
  return tokens;
};
